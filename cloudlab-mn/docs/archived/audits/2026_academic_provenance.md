# EMILY V3: ACADEMIC PROVENANCE (2026)

> **ABSTRACT:** This document validates the "Level 4" architecture against foundational research papers from the 2020-2025 era.
> **Date:** Jan 10, 2026.
> **Status:** CITATIONS VERIFIED.

## 1. INSTINCT: Liquid Neural Networks (LTC/NCP)

**Claim:** "Neural networks that adapt continuously after training."
**Source:** MIT CSAIL (Ramin Hasani, Daniela Rus).
**Key Papers:**

- **2021:** _"Liquid Time-constant Networks"_ (Hasani et al., Proc. AAAI).
- **2020:** _"Neural Circuit Policies Enabling Auditable Autonomy"_ (Lechner et al., Nature Machine Intelligence).
  **Why it matters:** Unlike Transformers which are static after training, LTCs are governed by differential equations that evolve over time, making them ideal for the "P4 Edge Node" handling continuous video/audio streams.

## 2. COGNITION: Generative Flow Networks (GFlowNets)

**Claim:** "Sampling diverse hypotheses proportional to reward, rather than maximizing reward."
**Source:** Mila (Yoshua Bengio).
**Key Papers:**

- **2021:** _"GFlowNet Foundations"_ (Bengio et al.).
- **2023:** _"GFlowNets for AI-Driven Scientific Discovery"_ (Jain et al.).
  **Why it matters:** Standard RL (PPO) converges to one single "best" answer. GFlowNets generate a _distribution_ of valid answers. This gives Emily "Creativity" (System 2) rather than just "Optimization."

## 3. MEMORY: Hyperdimensional Computing (HDC)

**Claim:** "Using 10,000-bit holographic vectors for noise-immune algebraic memory."
**Source:** Pentti Kanerva (Stanford/Redwood Center).
**Key Papers:**

- **2009:** _"Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors"_ (Kanerva, Cognitive Computation).
- **2023:** _"TorchHD: An Open Source Python Library for Hyperdimensional Computing"_ (Hern√°ndez-Cano et al.).
  **Why it matters:** HDC vectors can be "bound" and "bundled" algebraically (Addition = Superposition, Multiplication = Binding). You can subtract "Context" from "Memory" to retrieve "Object" instantly without searching.

## 4. CODE: Stack-Graphs (Neural AST)

**Claim:** "Incremental, precise code navigation using graph theory."
**Source:** GitHub Semantic Code Team (Douglas Creager).
**Key Papers/Talks:**

- **2021:** _"Precise code navigation for every repository"_ (GitHub Engineering Blog).
- **2023:** _"Stack Graphs: A formalism for name resolution"_ (Creager, Strange Loop).
  **Why it matters:** `tree-sitter` gives the syntax tree. `stack-graphs` turns that into a resolvable graph of definitions and references, allowing "Jump to Definition" across valid scopes, even in incomplete code.

## 5. INTUITION: Active Inference (Free Energy Principle)

**Claim:** "Minimizing surprise (variational free energy) through action."
**Source:** Karl Friston (UCL).
**Key Papers:**

- **2010:** _"The Free Energy Principle: A Unified Brain Theory?"_ (Friston, Nature Reviews Neuroscience).
  **Why it matters:** This defines _why_ Emily acts. She doesn't just "reply"; she acts to minimize the divergence between her internal world model and the external sensory inputs (User feedback).

## 6. INDEXING: Differentiable Search Index (DSI)

**Claim:** "A Transformer that hallucinates the Document ID directly."
**Source:** Google Research.
**Key Papers:**

- **2022:** _"Transformer Memory as a Differentiable Search Index"_ (Tay et al.).
  **Why it matters:** It removes the need for an external vector database (like Pinecone) for retrieval. The "Memory" is burned into the weights of the neural network itself.
