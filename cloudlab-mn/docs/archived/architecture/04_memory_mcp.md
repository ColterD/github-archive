# Emily V3: Memory MCP 2.0 Specification

## "Beyond Retrieval: The Neural State"

**Date:** January 10, 2026
**Author:** Database Specialist Agent (Gemini 3 Pro)
**Status:** DRAFT
**Reference:** `EMILY_V3_SYSTEM_LOG.md` (Analysis)

---

## 1. Executive Summary & Gap Analysis

**Current Stack (Standard 2025):**

- **Graph:** Graphiti (Temporal Knowledge Graph)
- **Vector:** Postgres (`pgvector` / `pg_embedding`)
- **Cache:** FalkorDB (Low-latency graph traversal)

**Critique:**
The current architecture relies on "Retrieval Augmented Generation" (RAG). This is brittle. It separates _storage_ from _cognition_. The database is dumb; the agent is smart. We fetch dead bytes and re-animate them in the context window.

- **Latency:** Vector search + Reranking is slow.
- **Fragmentation:** Graph and Vector are disjoint.
- **Stagnation:** Memories don't evolve unless explicitly rewritten.

**The 2.0 Shift:**
Move from **RAG** to **Neural State Persistence**. The memory system should be an active, living neural object, not a filing cabinet.

---

## 2. Core Architecture: The "Liquid" Memory Store

### 2.1. Replacing Postgres with Generative Indexing (DSI++)

Instead of storing embeddings in a vector index (HNSW/IVFFlat), we train a small, specialized Transformer (or Mamba) model to act as the index itself.

- **Concept:** **DSI (Differentiable Search Index)**.
- **Mechanism:**
  - Input: Query $q$.
  - Output: The unique identifier (Memory ID) of the relevant memory node directly.
  - Process: $P(id | q)$. The model "hallucinates" the valid Memory ID.
- **Implementation:**
  - Use a quantized **T5-Base** or **Mamba-3** architecture.
  - **Training:** The model is continually fine-tuned on `(Memory Content, Memory ID)` pairs.
  - **Benefit:** Zero-shot generalization. The model learns the _semantic topology_ of the user's life, mapping vague queries to specific IDs without scanning millions of vectors.
- **2026 Update:** We use **DSI++** with _dynamic corpus adaptation_. As new memories are added, we use Low-Rank Adapters (LoRA) to patch the index without full retraining.

### 2.2. The "Living" Database: Liquid Neural Networks (LNN) + Mamba

The "Short-term Memory" (Context Window) is replaced by a stateful neural layer.

- **Technology:** **Mamba (State Space Model)** or **Liquid Time-Constant (LTC) Networks**.
- **State Persistence:**
  - Standard transformers reset state ($KV$-cache) every generation.
  - **Memory MCP 2.0** maintains a persistent **Recurrent State ($h_t$)** that evolves over days/weeks.
  - The "database" is effectively the file `memory_state_tensor.pt` (the hidden state of the LNN), which captures the user's _interactions over time_ as a continuous function.
- **Usage:**
  - The Agent passes the user's prompt into the LNN.
  - The LNN updates its state $h_t \rightarrow h_{t+1}$.
  - The LNN outputs a "Gist" or "Context Vector" that is prepended to the LLM's query.
  - **Result:** Infinite context window compression. The model "feels" the history rather than reading it.

---

## 3. The "Sleep Spindles" Algorithm (Consolidation)

A concrete algorithm to transform raw logs into the Generative Index and Knowledge Graph.

**Trigger:**

- `Event`: `SystemIdle` (No user interaction > 15 mins) OR `DailyCycle` (3:00 AM Local).

**The 5-Phase Cycle:**

1.  **Phase 1: Replay (Hippocampal Replay)**

    - Access the **Episodic Buffer** (Redis/FalkorDB).
    - Select "Salient" interactions based on:
      - **Surprise:** High KL-Divergence from expected model output.
      - **Emotion:** Sentiment analysis bounds (very positive/negative).
    - _Action:_ Re-run these interactions through the DSI++ Trainer.

2.  **Phase 2: Abstraction (Graph Crystallization)**

    - Input: Salient interactions.
    - Process: Extract Subject-Predicate-Object triples.
    - _Check:_ Do these contradict existing Graphiti edges?
    - _Action:_ Update Temporal Graph. "Collapse" duplicate nodes (Entity Resolution).

3.  **Phase 3: Forgetting (Pruning)**

    - Identify Memory IDs that have not been generated by the DSI model in $N$ epochs (Low $P(id|q)$).
    - _Action:_ Move raw text to "Cold Storage" (S3/Blob). Delete vector embeddings. Keep Graph node (metadata only).

4.  **Phase 4: Optimization (Sleep Spindle)**

    - _Neural Consolidation:_ Merge the LoRA adapters from the day's DSI++ fine-tuning into the base model weights.
    - _Action:_ `merge_and_unload()`. Save new base model.

5.  **Phase 5: Wake (State Ready)**
    - Reload the Mamba State $h_t$ from disk.
    - Load new DSI model into GPU VRAM.
    - Clear Episodic Buffer.

---

## 4. Migration Plan (V3 -> V2.0)

1.  **Snapshot:** Dump Postgres `pgvector` table.
2.  **Training:** Train initial DSI model on `(chunk_text, chunk_id)` from Postgres.
3.  **Switchover:**
    - Old Path: `Query -> Embed -> PgVector -> Context`.
    - New Path: `Query -> DSI Model -> ID -> Graph Lookup -> Context`.
4.  **Install:** `mamba_ssm` and `liquid-net` python packages.
