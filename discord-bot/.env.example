# =============================================================================
# Discord Bot Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# All env vars are optional with sensible defaults unless marked [REQUIRED]
# =============================================================================

# -----------------------------------------------------------------------------
# Discord Configuration [REQUIRED]
# -----------------------------------------------------------------------------
# Bot display name (used in logs, embeds, and system messages)
BOT_NAME=Discord Bot

# Bot token from Discord Developer Portal
DISCORD_TOKEN=your_discord_bot_token_here

# Application Client ID from Discord Developer Portal
DISCORD_CLIENT_ID=your_client_id_here

# Application Client Secret from Discord Developer Portal (for OAuth)
# Found in OAuth2 section of your application
DISCORD_CLIENT_SECRET=your_client_secret_here

# Guild ID for development (guild commands update instantly)
# Leave empty for production (global commands take ~1hr to propagate)
DEV_GUILD_ID=your_dev_guild_id_here

# -----------------------------------------------------------------------------
# Dashboard Configuration
# -----------------------------------------------------------------------------
# Public URL for the dashboard (used for OAuth redirects)
# Default: http://localhost:3000 in development
DASHBOARD_URL=http://localhost:3000

# -----------------------------------------------------------------------------
# Environment
# -----------------------------------------------------------------------------
# 'development' | 'production'
NODE_ENV=development

# Logging level: DEBUG | INFO | WARN | ERROR
LOG_LEVEL=DEBUG

# -----------------------------------------------------------------------------
# LLM Configuration (Ollama)
# -----------------------------------------------------------------------------
# Ollama API endpoint
# Docker: http://ollama:11434 | Local: http://localhost:11434
OLLAMA_HOST=http://ollama:11434

# Primary model for chat (supports GGUF from HuggingFace)
LLM_MODEL=hf.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf:Q5_1

# Fallback model when VRAM is constrained
LLM_FALLBACK_MODEL=qwen2.5:7b

# Max tokens for responses
LLM_MAX_TOKENS=4096

# Temperature (0.0-2.0, lower = more focused, higher = more creative)
LLM_TEMPERATURE=0.7

# Request timeout in milliseconds (default: 5 minutes)
LLM_REQUEST_TIMEOUT=300000

# Keep model in GPU memory (seconds, -1 = forever)
LLM_KEEP_ALIVE=300

# Preload model on startup (true/false)
LLM_PRELOAD=true

# Sleep after inactivity (milliseconds, default: 5 minutes)
LLM_SLEEP_AFTER_MS=300000

# Use orchestrator for tool-aware conversations (true/false)
LLM_USE_ORCHESTRATOR=true

# HERETIC model specific settings (MoE optimization)
LLM_NUM_EXPERTS=5
LLM_REP_PEN=1.1
LLM_TEMP_CODING=0.6
LLM_TEMP_CREATIVE=1.0

# Context length - 64k is a good balance (memory system handles beyond via summarization)
LLM_CONTEXT_LENGTH=65536

# -----------------------------------------------------------------------------
# LLM Performance Options
# -----------------------------------------------------------------------------
# Batch size for prompt processing (higher = faster, uses more VRAM)
# Default: 1024, range: 1-2048+
LLM_NUM_BATCH=1024

# Number of CPU threads for parallel processing (0 = auto-detect all cores)
LLM_NUM_THREAD=0

# Enable flash attention for faster inference and lower VRAM usage
# Requires compatible GPU (RTX 4000 series fully supports this)
LLM_FLASH_ATTENTION=true

# Use memory-mapped files for faster model loading
# Reduces RAM usage and speeds up startup
LLM_MMAP=true

# Use memory locking to prevent swapping (requires elevated permissions)
# Generally not needed unless you're hitting swap issues
LLM_MLOCK=false

# Summarization model (runs on CPU to preserve GPU VRAM)
SUMMARIZATION_MODEL=qwen2.5:3b

# Embedding model for vector memory (runs on CPU)
EMBEDDING_MODEL=qwen3-embedding:0.6b

# -----------------------------------------------------------------------------
# Cloudflare Workers AI (Optional - for lightweight routing and embeddings)
# -----------------------------------------------------------------------------
# Offloads intent classification and embeddings from local GPU to Cloudflare
# Free tier: 10,000 neurons/day for text models, generous embedding limits
#
# HOW TO GET CREDENTIALS (as of December 2025):
# 1. Log in to Cloudflare Dashboard: https://dash.cloudflare.com
# 2. In the left sidebar, click "AI" â†’ "Workers AI"
# 3. Click "Use REST API" button
# 4. For API Token:
#    - Click "Create a Workers AI API Token"
#    - Review the prefilled permissions (Workers AI Read + Edit)
#    - Click "Create API Token" then "Copy API Token"
#    - Save it - you only see it once!
# 5. For Account ID:
#    - On the same page under "Get Account ID", copy the value
#
# Direct links:
# - Workers AI page: https://dash.cloudflare.com/?to=/:account/ai/workers-ai
# - API Tokens: https://dash.cloudflare.com/profile/api-tokens

# Account ID (32-character hex string)
CLOUDFLARE_ACCOUNT_ID=

# API Token with Workers AI permissions
CLOUDFLARE_API_TOKEN=

# Router model for intent classification (fast, cheap)
CLOUDFLARE_ROUTER_MODEL=@cf/ibm-granite/granite-4.0-h-micro

# Embedding model (1024 dimensions, matches Ollama qwen3-embedding:0.6b)
CLOUDFLARE_EMBEDDING_MODEL=@cf/qwen/qwen3-embedding-0.6b

# -----------------------------------------------------------------------------
# Cloudflare Worker Proxy (Optional - for edge-based low-latency AI)
# -----------------------------------------------------------------------------
# Deploy the Worker from cloudflare-worker/ for edge-based AI inference.
# This provides MUCH lower latency than the REST API (runs at nearest datacenter).
#
# HOW TO DEPLOY:
# 1. cd cloudflare-worker && npm install
# 2. npx wrangler login
# 3. npx wrangler secret put API_SECRET  (enter a strong random string)
# 4. npx wrangler deploy
# 5. Copy the URL and secret below

# Worker URL (e.g., https://discord-bot-ai-proxy.your-subdomain.workers.dev)
CLOUDFLARE_WORKER_URL=

# Secret for authenticating requests to the Worker
CLOUDFLARE_WORKER_SECRET=

# -----------------------------------------------------------------------------
# Valkey (Redis-compatible cache for active context)
# -----------------------------------------------------------------------------
VALKEY_URL=valkey://valkey:6379

# Conversation TTL in milliseconds (default: 30 minutes)
VALKEY_CONVERSATION_TTL_MS=1800000

# Key prefix for namespacing
VALKEY_KEY_PREFIX=discord-bot:

# -----------------------------------------------------------------------------
# ChromaDB (Vector Store for long-term memory)
# -----------------------------------------------------------------------------
CHROMA_URL=http://chromadb:8000
CHROMA_COLLECTION=memories

# -----------------------------------------------------------------------------
# Memory Configuration (Three-tier architecture)
# -----------------------------------------------------------------------------
# Master switch for memory system
MEMORY_ENABLED=true

# Summarization triggers
MEMORY_SUMMARIZE_AFTER_MESSAGES=15
MEMORY_SUMMARIZE_AFTER_IDLE_MS=1800000

# Context window allocation (tokens)
MEMORY_MAX_CONTEXT_TOKENS=4096
# Approximate characters per token for context budget (1-10, default 4)
MEMORY_CHARS_PER_TOKEN=4

# Relevance thresholds for memory retrieval (0.0-1.0)
MEMORY_PROFILE_THRESHOLD=0.4
MEMORY_EPISODIC_THRESHOLD=0.55

# Time decay - older memories weighted less (multiplier per day, 0.5-1.0)
MEMORY_TIME_DECAY_PER_DAY=0.98

# Minimum importance score for memories to be stored (0.0-1.0)
MEMORY_MIN_IMPORTANCE=0.3

# -----------------------------------------------------------------------------
# SearXNG (Web Search)
# -----------------------------------------------------------------------------
SEARXNG_URL=http://searxng:8080
SEARXNG_TIMEOUT=30000

# -----------------------------------------------------------------------------
# MCP (Model Context Protocol)
# -----------------------------------------------------------------------------
# Config file location for stdio-based MCP servers
MCP_CONFIG_PATH=./mcp-servers.json

# Timeouts in milliseconds
MCP_CONNECTION_TIMEOUT_MS=30000
MCP_REQUEST_TIMEOUT_MS=60000

# Docker MCP Gateway Configuration (Docker Desktop MCP Toolkit)
# Master switch to enable Docker MCP Gateway
DOCKER_MCP_ENABLED=false

# Transport type: "stdio" (recommended, spawns gateway process) or "http" (StreamableHTTP)
DOCKER_MCP_TRANSPORT=stdio

# HTTP Transport Settings (only used when DOCKER_MCP_TRANSPORT=http)
DOCKER_MCP_GATEWAY_URL=http://host.docker.internal:8811
DOCKER_MCP_GATEWAY_ENDPOINT=/mcp
DOCKER_MCP_BEARER_TOKEN=

# Reconnection settings
DOCKER_MCP_AUTO_RECONNECT=true
DOCKER_MCP_MAX_RECONNECT_ATTEMPTS=5

# -----------------------------------------------------------------------------
# Security / Permissions
# -----------------------------------------------------------------------------
# Bot permission system (comma-separated Discord user IDs)
BOT_OWNER_IDS=your_discord_user_id_here
BOT_ADMIN_IDS=
BOT_MODERATOR_IDS=

# Impersonation detection
SECURITY_IMPERSONATION_ENABLED=true
SECURITY_SIMILARITY_THRESHOLD=0.7

# Output sanitization - set to false for uncensored models
# When true, PII patterns (phone, email, SSN, etc.) in LLM output are redacted
SECURITY_REDACT_OUTPUT_PII=true
# When true, injection patterns (eval, system, webhooks) in LLM output are removed
SECURITY_FILTER_OUTPUT_INJECTIONS=true
# When true, security rules are prepended to the system prompt
# Set to false for uncensored models to remove guardrail instructions
SECURITY_SYSTEM_PROMPT_PREAMBLE=true
# When true, user prompts are validated for jailbreak/injection attempts
# Set to false for uncensored models that should accept any input
SECURITY_VALIDATE_PROMPTS=true

# -----------------------------------------------------------------------------
# ComfyUI (Image Generation)
# -----------------------------------------------------------------------------
# Master switch to enable/disable image generation feature
IMAGE_GENERATION_ENABLED=true

COMFYUI_URL=http://comfyui:8188
COMFYUI_MAX_QUEUE=5
COMFYUI_TIMEOUT=120000
COMFYUI_SLEEP_AFTER_MS=300000
COMFYUI_UNLOAD_ON_SLEEP=true

# ComfyUI is optimized in docker-compose.yml with:
# --fast: torch.compile for 10-30% faster generation
# --fp8_e4m3fn-text-enc: FP8 quantized text encoder (saves ~4GB VRAM)
# --lowvram: Offload to CPU when not generating
# --cuda-malloc: Better memory allocation
# PYTORCH_CUDA_ALLOC_CONF: Reduces VRAM fragmentation

# -----------------------------------------------------------------------------
# GPU / VRAM Configuration (for RTX 4090 with 24GB VRAM)
# -----------------------------------------------------------------------------
# Total VRAM available in MB (RTX 4090=24576, RTX 4080=16384, RTX 3090=24576)
GPU_TOTAL_VRAM_MB=24576

# Minimum free VRAM buffer to maintain (MB)
# 2.5GB ensures headroom for games/other apps (24GB - 2.5GB = 21.5GB for AI)
GPU_MIN_FREE_MB=2560

# VRAM usage thresholds (0.0-1.0)
GPU_WARNING_THRESHOLD=0.75
GPU_CRITICAL_THRESHOLD=0.90

# Estimated VRAM usage per task (MB)
GPU_LLM_VRAM_MB=14000
GPU_IMAGE_VRAM_MB=8000

# VRAM monitoring interval (ms)
GPU_POLL_INTERVAL_MS=5000

# Auto-unload LLM for image generation if VRAM is tight
GPU_AUTO_UNLOAD_FOR_IMAGES=true

# Inactivity timeout before unloading models from VRAM (milliseconds, default: 5 minutes)
COMFYUI_SLEEP_AFTER_MS=300000

# Whether to unload models when sleeping to free VRAM (true/false, default: true)
COMFYUI_UNLOAD_ON_SLEEP=true

# -----------------------------------------------------------------------------
# Health Monitor (Self-Healing Infrastructure)
# -----------------------------------------------------------------------------
# Health check interval in milliseconds (default: 30 seconds)
HEALTH_MONITOR_INTERVAL_MS=30000

# Alert threshold - time before escalating to owner (default: 5 minutes)
HEALTH_MONITOR_ALERT_THRESHOLD_MS=300000

# -----------------------------------------------------------------------------
# Rate Limiting
# -----------------------------------------------------------------------------
RATE_LIMIT_REQUESTS=10
RATE_LIMIT_WINDOW_MS=60000

# -----------------------------------------------------------------------------
# Testing Configuration
# -----------------------------------------------------------------------------
# Master switch for test mode (enables test channels and verbose logging)
TEST_MODE=false

# Webhook URL for automated testing
TEST_WEBHOOK_URL=

# Channels where bot responds to ALL messages (comma-separated channel IDs)
TEST_CHANNEL_IDS=

# Enable verbose logging for test channels
TEST_VERBOSE_LOGGING=false

# Fake token pattern for unit tests (should be obviously fake but valid format)
TEST_DISCORD_TOKEN_PATTERN=MTIzNDU2Nzg5MDEyMzQ1Njc4OTAx.G12345._test_token_not_real_do_not_use_

# -----------------------------------------------------------------------------
# Troubleshooting Notes
# -----------------------------------------------------------------------------
# If you see "Failed to connect to OAuth notifications" errors:
#   docker mcp feature disable mcp-oauth-dcr
# This is a known issue (GitHub: docker/mcp-gateway#245)
